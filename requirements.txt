# Core Deep Learning & LLM
torch>=2.0.0
transformers>=4.40.0
accelerate>=0.27.0
bitsandbytes>=0.42.0

# Unsloth for efficient fine-tuning
unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git

# Fine-tuning & LoRA
peft>=0.10.0
trl>=0.8.0

# Data Processing
datasets>=2.18.0
pandas>=2.0.0
scikit-learn>=1.3.0

# Evaluation Metrics
rouge-score>=0.1.2
bert-score>=0.3.13
evaluate>=0.4.1
nltk>=3.8.1

# Optimization
xformers>=0.0.24
triton>=2.1.0

# Utilities
tqdm>=4.66.0
matplotlib>=3.7.0
jupyter>=1.0.0
ipywidgets>=8.1.0

# Optional: For better performance
ninja>=1.11.1
packaging>=23.0
wheel>=0.40.0
